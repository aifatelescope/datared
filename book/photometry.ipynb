{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Photometry\n",
    "\n",
    "On this page we'll detect sources and perform simple aperture photometry (along other measurements) for each pre-reduced image.\n",
    "The same procedure could of course also be applied to coadded images.\n",
    "\n",
    "As before, you could copy or write the code shown below in a script, or alternatively directly download this page as a {download}`jupyter notebook <./photometry.ipynb>` file.\n",
    "\n",
    "To run the code, you'll need the module `dataredconfig.py`, as explained [here](./data.md).\n",
    "\n",
    "```{note}\n",
    "The photometry presented here is sufficient for our purposes, but remains extremely simple. Notably, it ignores the effect of variable seeing (in time and/or accross filters). The next steps to improve upon the minimal approach shown here would be to perform a PSF homogeneization or to use PSF-fitting.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataredconfig\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import astropy\n",
    "import astropy.visualization\n",
    "from astropy import units as u\n",
    "\n",
    "%matplotlib widget\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import ccdproc\n",
    "import photutils.background\n",
    "import photutils.aperture\n",
    "import photutils.segmentation\n",
    "import photutils.psf\n",
    "\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "from time import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll ignore some astropy warnings that get raised as our FITS headers (from NINA) are not 100% standards compliant.\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', category=astropy.wcs.FITSFixedWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Setting the correct path to the pre-reduced science frames to work on:\n",
    "#science_files_dir = dataredconfig.work_dir / \"LIGHT_PRERED\" # In case astrometric calibration was already performed before the pre-reduction\n",
    "science_files_dir = dataredconfig.work_dir / \"ASTROMETRY\"\n",
    "science_files = ccdproc.ImageFileCollection(science_files_dir, keywords=dataredconfig.ifc_header_keywords)\n",
    "\n",
    "# Where to write the catalogs:\n",
    "photometry_dir = dataredconfig.work_dir / \"PHOTOMETRY\"\n",
    "photometry_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Overview of all available files:\n",
    "science_files.summary\n",
    "# You may want to try this instead, for a better display:\n",
    "#science_files.summary.show_in_notebook()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: selecting the science frames to run on : \n",
    "object_to_process = \"HD92670\"\n",
    "\n",
    "science_files = science_files.filter(object=object_to_process)\n",
    "\n",
    "#science_files.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will measure photometry on all these files.\n",
    "But we start by picking one reference image, used to detect sources. It should be a good and deep exposure. We will also use this same image to check the overall procedure.\n",
    "\n",
    "Note that it doesn't have to be the first one. Select another index below if your first image is bad for some reason. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_image_filepath = science_files.files[0]\n",
    "ref_image = ccdproc.CCDData.read(ref_image_filepath)\n",
    "\n",
    "if(ref_image.wcs is None): # Performing forced photometry requires each frame to have a good WCS. This test is just to avoid obvious mistakes.\n",
    "    raise RuntimeError(\"The image has no WCS, make sure you specify the correct directory (i.e., with WCS) above!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating and subtracting the background\n",
    "\n",
    "The background estimation is run individually on every image (as the background might well change, e.g., due to clouds, light pollution, or stray light. We therefore start by defining a function for background estimation.\n",
    "Depending on the data to be processed, you might have to adjust the parameters of this function, to tune the \"flexibility\" of the background model to your science objective. The given default values are often ok, but certaintly not optimal for every situation. \n",
    "\n",
    "The typical compromise in background estimation is driven by the following. If the background model is too flexible, the background subtraction will take away some light from larger objects (such as galaxies, but also the central regions of star clusters). And if it is to rigid, the model won't be able to properly fit the structure that the background might have.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_background(image, box_size=300, filter_size=3):\n",
    "    \"\"\"Function to estimate the background and its RMS (root-mean-square deviation).\n",
    "    This is a simplifying wrapper around photutils, to expose and explain a bit the two key parameters box_size and filter_size.\n",
    "    \n",
    "    In short, the background is calculated by interpolating some sigma-clipped statistics evaluated on a relatively coarse grid.\n",
    "\n",
    "    Parameters:\n",
    "    image: a CCDData object\n",
    "    box_size: defines how fine the coarse grid is, in pixels.\n",
    "        The background gets estimated in each box of size box_size.\n",
    "        Ideally this is larger than the largest obects, but smaller than the background structure.\n",
    "    filter_size: an odd integer: window size of the median filter that gets applied to the measurements on the coarse grid.\n",
    "        This median filter helps for example to avoid \"bumps\" from large diffuse objects. \n",
    "\n",
    "    For more details, see https://photutils.readthedocs.io/en/stable/user_guide/background.html\n",
    "    \n",
    "    Returns:\n",
    "    background, background_rms: two numpy arrays\n",
    "    \"\"\"\n",
    "    \n",
    "    bkg = photutils.background.Background2D(\n",
    "        image.data,\n",
    "        box_size=(box_size, box_size),\n",
    "        filter_size=(filter_size, filter_size),\n",
    "        bkg_estimator=photutils.background.MedianBackground()\n",
    "        )\n",
    "    \n",
    "    return bkg.background, bkg.background_rms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply this to the reference image, to experiment with the parameters. \n",
    "\n",
    "If the background you want to subtract has relatively fine structures, try reducing box_size and simultaneouly increasing filter_size in the call below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We run this on the reference image\n",
    "background_box_size = 300\n",
    "background_filter_size = 3\n",
    "background, background_rms = estimate_background(ref_image, box_size=background_box_size, filter_size=background_filter_size)\n",
    "\n",
    "# Visualize the background image\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.title(\"Background model\")\n",
    "cbar = plt.imshow(background, origin='lower', cmap='Greys_r', interpolation='nearest')\n",
    "plt.colorbar(cbar, label=\"Pixel value [ADU]\")\n",
    "plt.xlabel(\"x [pixel]\")\n",
    "plt.ylabel(\"y [pixel]\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We subtract the background from the reference image\n",
    "ref_image_noback = ref_image.subtract(ccdproc.CCDData(background, unit=\"adu\"))\n",
    "\n",
    "# And visualize how \"flat\" the background-subtracted reference image is.\n",
    "# We smooth this slightly, to reduce noise and better see the structure.\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.title(\"Background-subtracted image, smoothed\")\n",
    "cbar = plt.imshow(gaussian_filter(ref_image_noback.data, sigma=3.0), origin='lower', cmap='Greys_r', interpolation='nearest', vmin=-20, vmax=20)\n",
    "plt.colorbar(cbar, label=\"Pixel value [ADU]\")\n",
    "plt.xlabel(\"x [pixel]\")\n",
    "plt.ylabel(\"y [pixel]\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above two visualizations will probably never look perfect, as background subtraction is difficult (way more sophisticated approaches exist). Explore if you can improve it by adjusting the background estimation parameters, otherwise keep the default values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In preparation for the source detection, we also have to define a \"threshold\" image for this reference frame, whose values are multiples of the estimated RMS of the background in each pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (RMS is an estimate of the standard deviation, so 3*RMS is \"3 sigma\" above the background)\n",
    "threshold = 3.0 * background_rms # this is an array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source detection and measurement\n",
    "\n",
    "We now detect sources (as groups of connected pixels above the threshold) and measure their exact positions (among other parameters), on the selected reference image. Let's nevertheless group all this in a function again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_measure(image, threshold):\n",
    "    \"\"\"Function to detect and measure sources.\n",
    "    This is close to what Source Extractor typically does.\n",
    "    \n",
    "    Parameters:\n",
    "    image: a CCDData object\n",
    "    threshold: a 2D-array with threshold values\n",
    "\n",
    "    Returns:\n",
    "    An Astropy Table with source positions (from  windowed centroids, \n",
    "    both in pixel and sky coordinates) and other measurements.\n",
    "    \"\"\"\n",
    "    data = image.data \n",
    "\n",
    "    # Filtering (convolving with a 2D Gaussian) for better source detection\n",
    "    kernel = photutils.segmentation.make_2dgaussian_kernel(fwhm=4.0, size=21)  # FWHM in pixels\n",
    "    convolved_data = astropy.convolution.convolve(data, kernel)\n",
    "\n",
    "    # Segmenting the image\n",
    "    finder = photutils.segmentation.SourceFinder(npixels=4, connectivity=4, progress_bar=False)\n",
    "    segment_map = finder(convolved_data, threshold)\n",
    "    #segment_map = finder(data, threshold) # One could also run this on the unfiltered image\n",
    "\n",
    "    # And measuring sources\n",
    "    source_catalog = photutils.segmentation.SourceCatalog(data, segment_map, convolved_data=convolved_data, wcs=image.wcs)\n",
    "    source_table = source_catalog.to_table(\n",
    "        columns=[\"xcentroid_win\", \"ycentroid_win\", \"sky_centroid_win\",\n",
    "                 \"fwhm\", \"max_value\", \"kron_flux\", \"segment_flux\", \"segment_area\"]\n",
    "        )\n",
    "\n",
    "    return astropy.table.Table(source_table) # We convert the QTable to a Table, better for later FITS writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this on the reference image\n",
    "ref_catalog = detect_and_measure(ref_image_noback, threshold)\n",
    "\n",
    "print(f\"Reference catalog: {len(ref_catalog)} sources detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running a detection, it's almost mandatory to check the result with a visualization! We do this with the following interactive Figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure that overplots the image and the positions of the detected sources\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = plt.subplot(projection=ref_image_noback.wcs)\n",
    "ax.imshow(ref_image_noback.data, origin='lower', cmap='Greys_r', interpolation='nearest',\n",
    "    norm=astropy.visualization.simple_norm(ref_image_noback.data, stretch=\"log\", vmin=-20, vmax=2000))\n",
    "ax.scatter(\n",
    "    ref_catalog[\"xcentroid_win\"],\n",
    "    ref_catalog[\"ycentroid_win\"],\n",
    "    transform=ax.get_transform('pixel'),\n",
    "    s=50, # The size of these markers is not related to any measurement apertures!\n",
    "    edgecolor='red', facecolor='none'\n",
    "    )\n",
    "ax.grid(color='white', ls='solid')\n",
    "ax.set_xlabel('RA')\n",
    "ax.set_ylabel('Dec')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zoom in and check if all relevant sources are detected. If there are issues, one might have to adjust the sensitivity of the detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your science objective, you can save CPU-time by setting a flux threshold to keep only those stars you actually care about, and not every tiny detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply some flux cut, to remove fainter sources if those are not needed:\n",
    "ref_catalog = ref_catalog[ref_catalog[\"kron_flux\"] > 10000]\n",
    "len(ref_catalog)\n",
    "\n",
    "# Now rerun the vizualisation above to see the effect, and adapt the threshold if needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reference, we write the reference catalog in this directory.\n",
    "# Note that this catalog also contains the aperture positions in sky coordinates.\n",
    "ref_catalog.write(photometry_dir / f\"ref_catalog_{object_to_process}.fits\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forced aperture photometry\n",
    "\n",
    "We'll now perform forced aperture photometry on all science images of our target, with a series of aperture sizes (radii given in arcseconds in the code below).\n",
    "This means that we use the sky coordinates of the sources detected in the reference frame to place the same apertures on all dithered exposures of the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We group all steps of the photometric measurement into one function:\n",
    "\n",
    "def measure_photometry(image, sky_positions, run_simple_fit=False, background_box_size=300, background_filter_size=3):\n",
    "    \"\"\"Function for forced aperture photometry given the provided sky_positions.\n",
    "    This function first estimates and subtracts the background from the image,\n",
    "    and performs aperture photometry both on the background-subtracted image and on the background alone.\n",
    "\n",
    "    Parameters:\n",
    "    image: a CCDData object\n",
    "    sky_positions: a list or column of aperture positions, in sky coordinates (i.e. as SkyCoord objects)\n",
    "    run_fit: if True, a simple 2D Gaussian fit is also run, providing FWHM and flux measurements. Take significantly more time.\n",
    "\n",
    "    Returns:\n",
    "    An Astropy Table with measurements for several aperture radii at the given positions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Estimating and subtracting the background:\n",
    "    tstart = time()    \n",
    "    background, _ = estimate_background(image, box_size=background_box_size, filter_size=background_filter_size)\n",
    "    image_noback = image.subtract(ccdproc.CCDData(background, unit=\"adu\"))\n",
    "    tend = time()\n",
    "    print(f\"Background subtraction took {tend-tstart:.2f} sec\")\n",
    "    \n",
    "    \n",
    "    # Aperture photometry\n",
    "    aperture_radii = [4, 6, 8, 10] # In arcsec\n",
    "    tstart = time()\n",
    "    # We generate a separate catalog for each aperture radius, and combine these afterwards\n",
    "    aperture_catalogs = []\n",
    "    for r in aperture_radii:\n",
    "\n",
    "        apertures = photutils.aperture.SkyCircularAperture(sky_positions, r=r*u.arcsec)\n",
    "\n",
    "        # Photometry on the background-subtracted image:\n",
    "        aperture_measurements = photutils.aperture.ApertureStats(image_noback.data, aperture=apertures, wcs=image_noback.wcs)\n",
    "        # Same photometry on the background image:\n",
    "        background_aperture_measurements = photutils.aperture.ApertureStats(background.data, aperture=apertures, wcs=image_noback.wcs)\n",
    "        \n",
    "        aperture_catalog = aperture_measurements.to_table(columns=[\"sum\", \"max\"])\n",
    "        background_aperture_catalog = background_aperture_measurements.to_table(columns=[\"sum\"])\n",
    "\n",
    "        # Note: ApertureStats also provides \"fwhm\" and \"elongation\", among other stats.\n",
    "        # While these measurements are fast and can be useful, they are computed from unweighted moments,\n",
    "        # and will therefore be strongly influenced by the aperture size and the noise.\n",
    "        # Just for reference, this is how to get them:\n",
    "        #aperture_catalog = aperture_measurements.to_table(columns=[\"fwhm\", \"elongation\", \"sum\"])        \n",
    "\n",
    "        # Rename columns by adding the aperture radius, to get for example \"sum_4\" instead of sum\n",
    "        for name in aperture_catalog.colnames:\n",
    "            aperture_catalog.rename_column(name, new_name=f\"{name}_{str(r)}\")\n",
    "        for name in background_aperture_catalog.colnames:\n",
    "            background_aperture_catalog.rename_column(name, new_name=f\"back_{name}_{str(r)}\")\n",
    "         \n",
    "        aperture_catalogs.append(aperture_catalog)\n",
    "        aperture_catalogs.append(background_aperture_catalog)\n",
    "    \n",
    "    # And now merge these catalogs into a single one:\n",
    "    merged_catalog = astropy.table.hstack(aperture_catalogs,\n",
    "        join_type=\"exact\", metadata_conflicts=\"silent\"\n",
    "        )\n",
    "    tend = time()\n",
    "    print(f\"Photometry took {tend-tstart:.2f} sec\")\n",
    "\n",
    "\n",
    "    # Optional simple 2D Gaussian fit for FWHM-measurement of sources:\n",
    "    if run_simple_fit:\n",
    "        tstart = time() \n",
    "        # Compute pixel-coordinate positions of the positions:\n",
    "        image_pixel_positions = astropy.wcs.utils.skycoord_to_pixel(sky_positions, image_noback.wcs)\n",
    "        xypos = np.array(image_pixel_positions).transpose() # Rearranging the data\n",
    "        # Add empty columns to hold the measurements:\n",
    "        merged_catalog.add_columns([np.nan, np.nan, np.nan], names=[\"fwhm_fit\", \"flux_fit\", \"q_fit\"])\n",
    "        # And loop over the sources.\n",
    "        # We need this loop as some sources might be outside of a particular exposure, raising exceptions.\n",
    "        for (i, pos) in enumerate(xypos):\n",
    "            try:\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\")\n",
    "                    fit_results = photutils.psf.fit_2dgaussian(image_noback.data, xypos=pos, fit_shape=15, fwhm=6, fix_fwhm=False).results\n",
    "                    merged_catalog[\"fwhm_fit\"][i] = fit_results[\"fwhm_fit\"][0]\n",
    "                    merged_catalog[\"flux_fit\"][i] = fit_results[\"flux_fit\"][0]\n",
    "                    merged_catalog[\"q_fit\"][i] = fit_results[\"qfit\"][0]\n",
    "            except Exception as e:\n",
    "                print(f\"Fit failed for source {i}\")\n",
    "        tend = time()\n",
    "        print(f\"2D Gaussian fitting took {tend-tstart:.2f} sec\")\n",
    "\n",
    "    return merged_catalog\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# As a test, we run this on the ref_image.\n",
    "# For this we read the ref image again, as the one in memory already has the background subtracted.\n",
    "ref_image = ccdproc.CCDData.read(ref_image_filepath)\n",
    "ref_photo_cat = measure_photometry(ref_image, ref_catalog[\"sky_centroid_win\"],\n",
    "                                   run_simple_fit=True,\n",
    "                                   background_box_size = background_box_size,\n",
    "                                   background_filter_size = background_filter_size\n",
    "                                   )\n",
    "print(ref_photo_cat.colnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize, the columns are the following.\n",
    " - `sum_x` is the background-subtracted flux within the aperture of radius `x` arcsec. This is what we'll use to compute magnitudes.\n",
    " - `max_x` is the maximum pixel value within the corresponding aperture. This informs us if some pixels are at or close to saturation.\n",
    " - `back_sum_x` is the flux of the background within the corresponding aperture. This could be used later to estimate the noise of the photometry.\n",
    " - `fwhm_fit`, `flux_fit`, `q_fit` are measurements from the optional 2D Gaussian fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chimney Plot\n",
    "\n",
    "As an illustration of how this photometric catalog can be used, we create a basic and very common scatter plot: the so-called \"chimney plot\", named after its shape. The plot shows instrumental magnitude against FWHM (or some other size measurement) of all detected sources.\n",
    "\n",
    "```{note} \n",
    "Potentially, your image contains both unresolved sources (stars) and resolved sources (e.g., galaxies). This plot should show you a clear \"stellar locus\" (i.e., where the stars are), and directly tell you the seeing of this particular image. Can you measure it? A potential fine \"smoke plume\" coming from your chimney is related to the saturation of the brightest stars.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just as an example, let's use the flux within the apertures of 6 arcseconds: \n",
    "ref_photo_cat[\"mag_6\"] = -2.5 * np.log10(ref_photo_cat[\"sum_6\"].value)\n",
    "\n",
    "# We create a scatter plot of FWHM versus mag, and color the datapoints according to the q_fit parameter (see legend).\n",
    "# High values of q_fit indicate that the source was not well fitted by the simple 2D-Gaussian.\n",
    "plt.figure()\n",
    "plt.scatter(\n",
    "    ref_photo_cat[\"fwhm_fit\"].value, \n",
    "    ref_photo_cat[\"mag_6\"], \n",
    "    c=ref_photo_cat[\"q_fit\"].value,\n",
    "    vmin=0.0, vmax=0.5,\n",
    "    cmap=\"jet\",\n",
    "    s=10\n",
    ")\n",
    "plt.gca().invert_yaxis() # Bright is at the top!\n",
    "plt.colorbar(label=\"Fit quality metric: sum(abs(fit residuals)) / (fitted flux)\")\n",
    "plt.xlabel(\"FWHM [pix]\")\n",
    "plt.xlim(0, 15)\n",
    "plt.ylabel(\"Instrumental magnitude\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing the measurements on all images \n",
    "\n",
    "Finally, we apply this measurement function to all images of the object.\n",
    "\n",
    "If you don't need the Gaussian fitting, you can run this faster by setting `run_simple_fit=False` below, in the call of `measure_photometry()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We read again the ref catalog from file:\n",
    "ref_catalog = astropy.table.Table.read(photometry_dir / f\"ref_catalog_{object_to_process}.fits\")\n",
    "\n",
    "\n",
    "n_files = len(science_files.summary)\n",
    "# And now we loop over all selected exposures:\n",
    "for (i, science_file) in enumerate(science_files.files):\n",
    "\n",
    "    image_filename = Path(science_file).stem\n",
    "    output_filepath = photometry_dir / f\"{image_filename}.fits\"\n",
    "\n",
    "    # Could use something like that to relaunch on new or selected files:\n",
    "    #if output_filepath.exists():\n",
    "    #    continue\n",
    "    # Or, alternatively:\n",
    "    #if i < 104:\n",
    "    #    continue\n",
    "    \n",
    "    print(f\"=== Processing image {i+1}/{n_files}: {image_filename} ===\")\n",
    "\n",
    "    image = ccdproc.CCDData.read(science_file)\n",
    "\n",
    "    # We build a dict containing some meta-information of the image taken from the FITS header.\n",
    "    # We'll later write this as meta-information of the catalogue.\n",
    "    meta_from_image = {key: image.header[key] for key in [\n",
    "        \"DATE-OBS\", \"EXPTIME\", \"IMAGETYP\", \"AIRMASS\", \"PIERSIDE\", \"FILTER\", \"OBJECT\",\n",
    "        \"FOCUSPOS\", \"FOCTEMP\", \"RA\", \"DEC\", \"SET-TEMP\", \"CCD-TEMP\", \"GAIN\", \"OFFSET\"\n",
    "        ]}\n",
    "    \n",
    "    # Now run the measurements:\n",
    "    catalog = measure_photometry(image, ref_catalog[\"sky_centroid_win\"],\n",
    "                                 run_simple_fit=False,\n",
    "                                 background_box_size = background_box_size,\n",
    "                                 background_filter_size = background_filter_size\n",
    "                                 )\n",
    "\n",
    "    # Add the information from the FITS header\n",
    "    catalog.meta.update(meta_from_image)\n",
    "\n",
    "    # Could copy a few columns from the reference catalog\n",
    "    # But no need to do this here: we keep the reference catalog, and will use it later. \n",
    "    #catalog[\"sky_centroid_win\"] = ref_catalog[\"sky_centroid_win\"]\n",
    "\n",
    "    # And write all this as a FITS table\n",
    "    catalog.write(output_filepath, overwrite=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "Again, this might take a while to run if you have many exposures. In particular if you're analysing a transit, you can leave this cell running and start setting up the next notebook.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datared2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

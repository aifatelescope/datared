{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Light curve\n",
    "\n",
    "In this notebook we'll assemble the light curve of a star, calibrated against other stars of the same field, with the purpose of uncovering an exoplanet transit.\n",
    "\n",
    "As usual, you can download this page as a {download}`jupyter notebook <./lightcurve.ipynb>` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataredconfig\n",
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import astropy\n",
    "import astropy.table\n",
    "import astropy.visualization\n",
    "from astropy import units as u\n",
    "\n",
    "import datetime\n",
    "\n",
    "%matplotlib widget\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import ccdproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll ignore some astropy warnings that get raised as our FITS headers (from NINA) are not 100% standards compliant.\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', category=astropy.wcs.FITSFixedWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photometry_dir = dataredconfig.work_dir / \"PHOTOMETRY\"\n",
    "\n",
    "object_to_process = \"HD92670\"\n",
    "\n",
    "catalog_filepaths = sorted(list(photometry_dir.glob('*.fits')))\n",
    "catalogs = []\n",
    "\n",
    "# We'll read all catalogs in, can keep only those matching the desired object in the above list.\n",
    "for catalog_filepath in catalog_filepaths:\n",
    "    \n",
    "    catalog = astropy.table.Table.read(catalog_filepath)\n",
    "\n",
    "    # We select the photometric catalogs of our object:  \n",
    "    if \"OBJECT\" in catalog.meta:\n",
    "        if catalog.meta[\"OBJECT\"] == object_to_process:\n",
    "            print(f\"{catalog_filepath} : {catalog.meta}\")\n",
    "            catalogs.append(catalog)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We combine these catalogs into a single table, in \"depth\": columns will be 2D, where the second dimension is time.\n",
    "catalog = astropy.table.dstack(catalogs, join_type=\"exact\", metadata_conflicts=\"silent\")\n",
    "\n",
    "# We also produce a list of datetime objects, from the FITS headers:\n",
    "date_strings = [c.meta[\"DATE-OBS\"] for c in catalogs]\n",
    "dates = [datetime.datetime.fromisoformat(date) for date in date_strings]\n",
    "\n",
    "# And while we are at it, same for the airmass:\n",
    "airmasses = [c.meta[\"AIRMASS\"] for c in catalogs]\n",
    "\n",
    "# We read the reference catalog, as this one contains the position of each star\n",
    "ref_catalog = astropy.table.Table.read(photometry_dir / f\"ref_catalog_{object_to_process}.fits\")\n",
    "assert len(ref_catalog) == len(catalog) # Just a check that these are indeed of same length\n",
    "\n",
    "# We copy the positions from the reference catalog over to our combined catalog:\n",
    "catalog[\"sky_centroid_win\"] = ref_catalog[\"sky_centroid_win\"]\n",
    "\n",
    "n_epochs = len(dates)\n",
    "print(f\"Number of epochs: {n_epochs}\")\n",
    "\n",
    "# And just for information, we print the column names and column shapes of our \"combined\" catalogue.\n",
    "# Indeed, most of the \"columns\" are in fact 2D arrays: \n",
    "for colname in catalog.colnames:\n",
    "    print(f\"{colname}: {catalog[colname].shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional cell to create a binned version of the data catalog\n",
    "\n",
    "binsize = 10 # How many measurements to group in each bin\n",
    "\n",
    "n_epochs_binnable = n_epochs - (n_epochs % binsize) # This is number of epochs we can use (largest possible integer multiple of binsize)\n",
    "print(f\"With bin size {binsize} we can use {n_epochs_binnable} of the {n_epochs} epochs.\")\n",
    "\n",
    "# First, we create a list of binned dates: we take the \"mean\" of the epochs in each bin.\n",
    "dates_reshape = np.reshape(dates[:n_epochs_binnable], (-1, binsize))\n",
    "dates_binned = [pd.to_datetime(pd.Series(bin)).mean() for bin in dates_reshape]\n",
    "\n",
    "# Then we create a \"binned\" catalog, starting from a full copy.\n",
    "catalog_binned = catalog.copy()\n",
    "\n",
    "# Define how to process columns:\n",
    "colnames_to_sum = [\"sum_4\", \"sum_6\", \"sum_8\", \"sum_10\", \"back_sum_4\", \"back_sum_6\", \"back_sum_8\", \"back_sum_10\"] # Could add \"flux_fit\" if available\n",
    "colnames_to_mean = [] # Could add \"fwhm_fit\", \"q_fit\"\n",
    "colnames_to_median = [\"max_4\", \"max_6\", \"max_8\", \"max_10\"]\n",
    "\n",
    "for colname in colnames_to_sum + colnames_to_mean + colnames_to_median:\n",
    "\n",
    "    original_column_shape = catalog[colname].shape\n",
    "    if len(original_column_shape) != 2:\n",
    "        raise(RuntimeError(f\"Column {colname} has shape {original_column_shape} and can't be binned.\"))\n",
    "    \n",
    "    nb_sources = original_column_shape[0]\n",
    "    column_reshape = np.reshape(catalog[colname][:,:n_epochs_binnable], (nb_sources, -1, binsize)) # This is now 3D : (source index, nb of binned epochs, binsize)\n",
    "\n",
    "    if colname in colnames_to_sum:\n",
    "        column_binned = np.sum(column_reshape, axis=2) #sum within bins\n",
    "    elif colname in colnames_to_mean:\n",
    "        column_binned = np.mean(column_reshape, axis=2) # mean within bins\n",
    "    elif colname in colnames_to_median:\n",
    "        column_binned = np.median(column_reshape, axis=2) # median within bins\n",
    "\n",
    "    catalog_binned[colname] = column_binned\n",
    "\n",
    "# For information, the column names and column shapes of the \"binned\" catalogue are:\n",
    "for colname in catalog_binned.colnames:\n",
    "    print(f\"{colname}: {catalog_binned[colname].shape}\")\n",
    "print(f\"Number of epochs in binned catalog: {len(dates_binned)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We display an image of the field, overplotting the \"source indices\" corresponding to rows of our catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load one of the images, it does not have to be a specific one.\n",
    "light_prered_dir = dataredconfig.work_dir / \"ASTROMETRY\"\n",
    "science_files = ccdproc.ImageFileCollection(light_prered_dir, keywords=dataredconfig.ifc_header_keywords)\n",
    "science_files = science_files.filter(object=object_to_process)\n",
    "image_path = science_files.files[0]\n",
    "image = ccdproc.CCDData.read(image_path, unit=\"adu\")\n",
    "image.data -= np.median(image.data) # Quick sky subtraction\n",
    "\n",
    "# We test that the selected image does have WCS information, to prevent unexpected errors being raised further below. \n",
    "if(image.wcs is None):\n",
    "    raise RuntimeError(\"This image has no WCS, make sure you specify the correct directory (i.e., with WCS) above!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# And now create the figure\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = plt.subplot(projection=image.wcs)\n",
    "ax.imshow(image.data, origin='lower', cmap='Greys_r', interpolation='nearest',\n",
    "    norm=astropy.visualization.simple_norm(image.data, stretch=\"sqrt\", vmin=-20, vmax=500))\n",
    "ax.scatter(\n",
    "    catalog[\"sky_centroid_win\"].ra.degree,\n",
    "    catalog[\"sky_centroid_win\"].dec.degree,\n",
    "    transform=ax.get_transform('world'),\n",
    "    s=50, # The size of these markers is not related to any measurement apertures!\n",
    "    edgecolor='red', facecolor='none'\n",
    "    )\n",
    "for line in catalog:\n",
    "    ax.text(\n",
    "        x=line[\"sky_centroid_win\"].ra.degree,\n",
    "        y=line[\"sky_centroid_win\"].dec.degree,\n",
    "        s=str(line.index),\n",
    "        transform=ax.get_transform('world'),\n",
    "        color=\"cyan\"\n",
    "        )\n",
    "ax.grid(color='white', ls='solid')\n",
    "ax.coords[0].set_axislabel('RA')\n",
    "ax.coords[1].set_axislabel('Dec')\n",
    "#ax.coords[0].set_ticks(spacing=5.*u.arcmin)\n",
    "#ax.coords[1].set_ticks(spacing=5.*u.arcmin)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now visualize the raw light curves, to get a first impression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which stars (\"source indices\") should we plot?\n",
    "indices_show = []\n",
    "\n",
    "# We compute an \"instrumental magnitude\" from our flux measurements:\n",
    "catalog[\"instr_mag\"] = -2.5 * np.log10(catalog[\"sum_10\"].value) # this is a \"2D\" column: (source index, date)\n",
    "\n",
    "plt.figure()\n",
    "ax = plt.subplot()\n",
    "\n",
    "# Plot some field stars in grey:\n",
    "for index in indices_show:\n",
    "    ax.plot(dates, catalog[\"instr_mag\"][index], lw=1, label=index)\n",
    "\n",
    "ax.invert_yaxis() # Needed, as we show a magnitude on y.\n",
    "\n",
    "# Some advanced settings to help getting a nice format of the date axis labels:\n",
    "ax.xaxis.set_major_formatter(matplotlib.dates.ConciseDateFormatter(ax.xaxis.get_major_locator()))\n",
    "\n",
    "ax.set_ylabel(\"Instrumental magnitude\")\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Question\n",
    "Do you observe any trends in these light curves, and are these common to all stars? Comment on what could cause these trends.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration\n",
    "\n",
    "To reveal the transit itself, some empirical calibration of the flux (or magnitude) measured in each exposure is needed.\n",
    "\n",
    "```{admonition} Question\n",
    "What would be the properties of good reference stars to use for the calibration?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the calibration (and to produce a nice figure afterwards) some \"settings\":\n",
    "\n",
    "target_index = 611 # Which index in the field image from above corresponds to the target star (the one that shows the transit)?\n",
    "mean_instr_mag_limit = -15.0\n",
    "std_instr_mag_limit = 0.015\n",
    "nbr_images_baseline = 20 # how many images first baseline (to set magnitude)\n",
    "n_bright_stars_calib = 20 # How many of the brightest stars to use for calibration\n",
    "n_bright_stars_show = 8 # How many of the brightest stars should be plotted \n",
    "title_str = \"HAT-P-23 b, Moon 34% @ 119°, depth 10.5 ppt\"\n",
    "\n",
    "flux_colname = \"sum_6\"\n",
    "\n",
    "\n",
    "\n",
    "# Some values from the literature:\n",
    "target_mag = 11.9 # What magnitude does this star have?\n",
    "transit_depth_ppt = 10.5 # How deep is the transit (in parts per thousands \"ppt\") \n",
    "ingress_datetime = datetime.datetime.fromisoformat(\"2024-08-27T20:31\") # Both in UTC\n",
    "egress_datetime = datetime.datetime.fromisoformat(\"2024-08-27T22:42\")\n",
    "\n",
    "\n",
    "transit_depth_mag = 2.5*np.log10(1.0/(1.0-transit_depth_ppt/1000.0))\n",
    "\n",
    "\n",
    "\n",
    "# We compute the instrumental magnitude, for the desired aperture size (Maybe you did this already, but it doesn't harm)\n",
    "catalog[\"instr_mag\"] = -2.5 * np.log10(catalog[flux_colname].value) # a \"2D\" column (index, date)\n",
    "catalog[\"median_instr_mag\"] = np.nanmedian(catalog[\"instr_mag\"], axis=1) # one value per source\n",
    "catalog[\"std_instr_mag\"] = np.std(catalog[\"instr_mag\"].value, axis=1) # this is just a 1Dcolumn: (source index)\n",
    "\n",
    "target_center_pos = catalog[\"sky_centroid_win\"][target_index]\n",
    "catalog[\"separation\"] = target_center_pos.separation(catalog[\"sky_centroid_win\"])\n",
    "\n",
    "\n",
    "\n",
    "catalog[\"flux_sort_indices\"] = np.argsort(catalog[\"median_instr_mag\"])\n",
    "\n",
    "flux_sort_indices = [i for i in catalog[\"flux_sort_indices\"] if \n",
    "                     catalog[\"median_instr_mag\"][i] > mean_instr_mag_limit and \n",
    "                     catalog[\"std_instr_mag\"][i] < std_instr_mag_limit and\n",
    "                     catalog[\"separation\"][i] < 0.1 and\n",
    "                     i != target_index\n",
    "                     #np.abs(catalog[\"med_diff\"][i] - -0.01764350479282406) < 0.005\n",
    "                    ]\n",
    "\n",
    "print(\"Number of stars remaining:\", len(flux_sort_indices))\n",
    "indices_calib = flux_sort_indices[0:n_bright_stars_calib]\n",
    "indices_show = flux_sort_indices[0:n_bright_stars_show]\n",
    "\n",
    "#catalog[\"flux_sort_indices\"] = np.argsort(catalog[\"median_instr_mag\"])\n",
    "#indices_calib = catalog[\"flux_sort_indices\"][0:n_bright_stars_calib]\n",
    "#indices_show = catalog[\"flux_sort_indices\"][0:n_bright_stars_show]\n",
    "\n",
    "#indices_calib = [118, 112, 103, 136, 141]\n",
    "#indices_show = [118, 112, 103, 136, 141]\n",
    "\n",
    "catalog[\"delta_instr_mag\"] = catalog[\"instr_mag\"] - np.expand_dims(catalog[\"median_instr_mag\"], axis=1) # 2D - 1D -> 2D (index, date)\n",
    "\n",
    "same_calib_for_all = np.nanmedian(catalog[\"delta_instr_mag\"][indices_calib], axis=0)\n",
    "# Normalize this:\n",
    "same_calib_for_all -= np.mean(same_calib_for_all)\n",
    "# And make it a colum of the catalog\n",
    "catalog[\"rel_calib\"] = np.tile(same_calib_for_all, (len(catalog), 1))\n",
    "\n",
    "catalog[\"calib_instr_mag\"] = catalog[\"instr_mag\"] - catalog[\"rel_calib\"]\n",
    "\n",
    "\n",
    "zero_point = target_mag - np.nanmedian(catalog[\"calib_instr_mag\"].value[target_index, 0:nbr_images_baseline])\n",
    "print(np.nanmedian(catalog[\"calib_instr_mag\"].value[target_index, 0:nbr_images_baseline]))\n",
    "print(f\"Zero-point: {zero_point}\")\n",
    "catalog[\"mag\"] = catalog[\"calib_instr_mag\"] + zero_point\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Same for the binned catalog\n",
    "\n",
    "\n",
    "nbr_images_baseline = 4\n",
    "\n",
    "catalog_binned[\"instr_mag\"] = -2.5 * np.log10(catalog_binned[flux_colname].value) # a \"2D\" column (index, date)\n",
    "catalog_binned[\"median_instr_mag\"] = np.nanmedian(catalog_binned[\"instr_mag\"], axis=1) # one value per source\n",
    "\n",
    "#catalog_binned[\"flux_sort_indices\"] = np.argsort(catalog_binned[\"median_instr_mag\"])\n",
    "#indices_calib = catalog_binned[\"flux_sort_indices\"][0:n_bright_stars_calib]\n",
    "#indices_show = catalog_binned[\"flux_sort_indices\"][0:n_bright_stars_show]\n",
    "\n",
    "#indices_calib = [118, 112, 103, 136, 141]\n",
    "#indices_show = [118, 112, 103, 136, 141]\n",
    "\n",
    "catalog_binned[\"delta_instr_mag\"] = catalog_binned[\"instr_mag\"] - np.expand_dims(catalog_binned[\"median_instr_mag\"], axis=1) # 2D - 1D -> 2D (index, date)\n",
    "\n",
    "same_calib_for_all = np.nanmedian(catalog_binned[\"delta_instr_mag\"][indices_calib], axis=0)\n",
    "# Normalize this:\n",
    "same_calib_for_all -= np.mean(same_calib_for_all)\n",
    "# And make it a colum of the catalog\n",
    "catalog_binned[\"rel_calib\"] = np.tile(same_calib_for_all, (len(catalog_binned), 1))\n",
    "\n",
    "catalog_binned[\"calib_instr_mag\"] = catalog_binned[\"instr_mag\"] - catalog_binned[\"rel_calib\"]\n",
    "\n",
    "\n",
    "zero_point = target_mag - np.nanmedian(catalog_binned[\"calib_instr_mag\"].value[target_index, 0:nbr_images_baseline])\n",
    "print(np.nanmedian(catalog_binned[\"calib_instr_mag\"].value[target_index, 0:nbr_images_baseline]))\n",
    "print(f\"Zero-point: {zero_point}\")\n",
    "catalog_binned[\"mag\"] = catalog_binned[\"calib_instr_mag\"] + zero_point\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                    \n",
    "plt.figure()#figsize=(6, 6))\n",
    "ax = plt.subplot()\n",
    "ax.axvline(ingress_datetime, lw=1)\n",
    "ax.axvline(egress_datetime, lw=1)\n",
    "ax.axhline(target_mag, lw=1, ls=\"--\")\n",
    "ax.axhline(target_mag + transit_depth_mag, lw=1, ls=\":\")\n",
    "\n",
    "# Plot some field stars in grey:\n",
    "for (i, index) in enumerate(indices_show):\n",
    "    if index == target_index:\n",
    "        continue\n",
    "    ax.plot(\n",
    "    dates, catalog[\"mag\"][index] - (catalog[\"median_instr_mag\"].value[index] - catalog[\"median_instr_mag\"].value[target_index]) - 0.012*(i+3), \n",
    "    #dates, catalog[\"mag\"][index] - catalog[\"calib\"][target_index], \n",
    "    lw=0.3\n",
    "    )\n",
    "\n",
    "# Plot the target in red:\n",
    "ax.plot(\n",
    "    dates, catalog[\"mag\"][target_index], \n",
    "    color=\"red\", ls=\"None\", marker=\"o\", markersize=1, alpha=0.2, lw=0.1\n",
    "    )\n",
    "\n",
    "# compute error bars\n",
    "\n",
    "gain =  0.376 # e/ADU\n",
    "flux = catalog_binned[flux_colname][target_index]\n",
    "#flux = catalog[flux_colname][target_index]\n",
    "flux_err = np.sqrt(flux) / (np.sqrt(gain))\n",
    "mag_err = 2.5 * np.log10(1 + (flux_err/flux))\n",
    "\n",
    "\n",
    "ax.errorbar(dates_binned, catalog_binned[\"mag\"][target_index], yerr=mag_err, ls=\"None\", marker=\"o\", color=\"red\", markersize=2)\n",
    "#ax.errorbar(dates, catalog[\"mag\"][target_index], yerr=mag_err, ls=\"None\", marker=\"o\", color=\"red\", markersize=2)\n",
    "\n",
    "\n",
    "\n",
    "ax.invert_yaxis()\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "color=\"purple\"\n",
    "ax2.set_ylabel('Airmass', color=color)\n",
    "ax2.plot(dates, airmasses, color=color, lw=1, ls=\"-.\")\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "\n",
    "ax.xaxis.set_major_formatter(matplotlib.dates.ConciseDateFormatter(ax.xaxis.get_major_locator()))\n",
    "#plt.colorbar(label=f\"Separation to target center in {cat['separation'].unit}\")\n",
    "#plt.xlabel(\"Date\")\n",
    "ax.set_ylabel(\"r Magnitude\", color=\"red\")\n",
    "ax.set_xlabel(\"UTC\")\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.title(title_str)\n",
    "plt.tight_layout()\n",
    "#plt.show()\n",
    "plt.savefig(\"2024-08-27_HAT-P-23_b.pdf\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datared2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

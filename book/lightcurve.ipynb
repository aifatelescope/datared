{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exoplanet transit light curve\n",
    "\n",
    "In this notebook we'll assemble the light curve of a star, calibrated against other stars of the same field, with the purpose of uncovering an exoplanet transit.\n",
    "\n",
    "As usual, you can download this page as a {download}`jupyter notebook <./lightcurve.ipynb>` file.\n",
    "\n",
    "```{note}\n",
    "In case you have been working on an AIfA lab room computer until this point, you might be interested in moving to a personal laptop (for example) for these last steps.\n",
    "All that we'll need from here on is the content of the `PHOTOMETRY` directory (including the reference catalog), as well as **one** of the pre-reduced images that has a WCS-transformation in its header (from `ASTROMETRY` of `LIGHT_PRERED`, depending on how the processing was run). This is a relatively small amount of data (compared to the raw or pre-reduced images), so you could consider transferring just these files to your own computer. The CPU requirements for these last steps will also be negligible. Make sure to install the right python environment, following [](./installation.md). There is no need for `astrometry.net` at this stage.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataredconfig\n",
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import astropy\n",
    "import astropy.table\n",
    "import astropy.visualization\n",
    "import astropy.coordinates\n",
    "from astropy import units as u\n",
    "\n",
    "import datetime\n",
    "\n",
    "%matplotlib widget\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import ccdproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll ignore some astropy warnings that get raised as our FITS headers (from NINA) are not 100% standards compliant.\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', category=astropy.wcs.FITSFixedWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photometry_dir = dataredconfig.work_dir / \"PHOTOMETRY\"\n",
    "\n",
    "object_to_process = \"HD92670\"\n",
    "\n",
    "catalog_filepaths = sorted(list(photometry_dir.glob('*.fits')))\n",
    "catalogs = []\n",
    "\n",
    "# We'll read all catalogs in, can keep only those matching the desired object in the above list.\n",
    "for catalog_filepath in catalog_filepaths:\n",
    "    \n",
    "    catalog = astropy.table.Table.read(catalog_filepath)\n",
    "\n",
    "    # We select the photometric catalogs of our object:  \n",
    "    if \"OBJECT\" in catalog.meta:\n",
    "        if catalog.meta[\"OBJECT\"] == object_to_process:\n",
    "            print(f\"{catalog_filepath} : {catalog.meta}\")\n",
    "            catalogs.append(catalog)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We combine these catalogs into a single table, in \"depth\": columns will be 2D, where the second dimension is time.\n",
    "catalog = astropy.table.dstack(catalogs, join_type=\"exact\", metadata_conflicts=\"silent\")\n",
    "\n",
    "# We also produce a list of datetime objects, from the FITS headers:\n",
    "date_strings = [c.meta[\"DATE-OBS\"] for c in catalogs]\n",
    "dates = [datetime.datetime.fromisoformat(date) for date in date_strings]\n",
    "\n",
    "# And while we are at it, same for the airmass:\n",
    "airmasses = [c.meta[\"AIRMASS\"] for c in catalogs]\n",
    "\n",
    "# We read the reference catalog, as this one contains the position of each star\n",
    "ref_catalog = astropy.table.Table.read(photometry_dir / f\"ref_catalog_{object_to_process}.fits\")\n",
    "assert len(ref_catalog) == len(catalog) # Just a check that these are indeed of same length\n",
    "\n",
    "# We copy the positions from the reference catalog over to our combined catalog:\n",
    "catalog[\"sky_centroid_win\"] = ref_catalog[\"sky_centroid_win\"]\n",
    "\n",
    "n_epochs = len(dates)\n",
    "print(f\"Number of epochs: {n_epochs}\")\n",
    "\n",
    "# And just for information, we print the column names and column shapes of our \"combined\" catalogue.\n",
    "# Indeed, most of the \"columns\" are in fact 2D arrays:\n",
    "print(\"Column names and shapes:\")  \n",
    "for colname in catalog.colnames:\n",
    "    print(f\"{colname}: {catalog[colname].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, the data is in structure that is convenient for plotting and calibration: the epochs are in a list of datetime objects (`dates`), and all the corresponding flux measurements are in the `catalog`.\n",
    "\n",
    "For noisy observations, it might be interesting to \"bin\" the light curves, that is to sum the flux of each source in groups of n exposures. This increases the signal to noise ratio of the datapoints, at the cost of decreasing the sampling along the time axis.\n",
    "Before starting with the further analysis and calibration, we provide below a piece of code that creates a `catalog_binned` and a corresponding `dates_binned`, that you might want to use instead of `catalog` and `dates`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional cell to create a binned version of the data catalog\n",
    "\n",
    "binsize = 10 # How many measurements to group in each bin\n",
    "\n",
    "n_epochs_binnable = n_epochs - (n_epochs % binsize) # Number of epochs we can use (largest possible integer multiple of binsize)\n",
    "print(f\"With bin size {binsize} we can use {n_epochs_binnable} of the {n_epochs} epochs.\")\n",
    "\n",
    "# First, we create a list of binned dates: we take the \"mean\" of the epochs in each bin.\n",
    "dates_reshape = np.reshape(dates[:n_epochs_binnable], (-1, binsize))\n",
    "dates_binned = [pd.to_datetime(pd.Series(bin)).mean() for bin in dates_reshape]\n",
    "\n",
    "# Then we create a \"binned\" catalog, starting from a full copy.\n",
    "catalog_binned = catalog.copy()\n",
    "\n",
    "# Define how to process columns:\n",
    "colnames_to_sum = [\"sum_4\", \"sum_6\", \"sum_8\", \"sum_10\", \"back_sum_4\", \"back_sum_6\", \"back_sum_8\", \"back_sum_10\"] # Could add \"flux_fit\" if available\n",
    "colnames_to_mean = [] # Could add \"fwhm_fit\", \"q_fit\"\n",
    "colnames_to_median = [\"max_4\", \"max_6\", \"max_8\", \"max_10\"]\n",
    "\n",
    "for colname in colnames_to_sum + colnames_to_mean + colnames_to_median:\n",
    "\n",
    "    original_column_shape = catalog[colname].shape\n",
    "    if len(original_column_shape) != 2:\n",
    "        raise(RuntimeError(f\"Column {colname} has shape {original_column_shape} and can't be binned.\"))\n",
    "    \n",
    "    nb_sources = original_column_shape[0]\n",
    "    column_reshape = np.reshape(catalog[colname][:,:n_epochs_binnable], (nb_sources, -1, binsize)) # This is now 3D : (source index, nb of binned epochs, binsize)\n",
    "\n",
    "    if colname in colnames_to_sum:\n",
    "        column_binned = np.sum(column_reshape, axis=2) #sum within bins\n",
    "    elif colname in colnames_to_mean:\n",
    "        column_binned = np.mean(column_reshape, axis=2) # mean within bins\n",
    "    elif colname in colnames_to_median:\n",
    "        column_binned = np.median(column_reshape, axis=2) # median within bins\n",
    "\n",
    "    catalog_binned[colname] = column_binned\n",
    "\n",
    "# For information, the column names and column shapes of the \"binned\" catalogue are:\n",
    "for colname in catalog_binned.colnames:\n",
    "    print(f\"{colname}: {catalog_binned[colname].shape}\")\n",
    "print(f\"Number of epochs in binned catalog: {len(dates_binned)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the field of view\n",
    "\n",
    "We now display an image of the field, overplotting the \"source indices\" corresponding to rows of our catalog.\n",
    "\n",
    "This will allow us to identify the target and reference stars. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load one of the images, it does not have to be a specific one.\n",
    "light_prered_dir = dataredconfig.work_dir / \"ASTROMETRY\"\n",
    "science_files = ccdproc.ImageFileCollection(light_prered_dir, keywords=dataredconfig.ifc_header_keywords)\n",
    "science_files = science_files.filter(object=object_to_process)\n",
    "image_path = science_files.files[0]\n",
    "image = ccdproc.CCDData.read(image_path, unit=\"adu\")\n",
    "image.data -= np.median(image.data) # Quick sky subtraction\n",
    "\n",
    "# We test that the selected image does have WCS information, to prevent unexpected errors being raised further below. \n",
    "if(image.wcs is None):\n",
    "    raise RuntimeError(\"This image has no WCS, make sure you specify the correct directory (i.e., with WCS) above!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now create the figure\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = plt.subplot(projection=image.wcs)\n",
    "ax.imshow(image.data, origin='lower', cmap='Greys_r', interpolation='nearest',\n",
    "    norm=astropy.visualization.simple_norm(image.data, stretch=\"sqrt\", vmin=-20, vmax=500))\n",
    "ax.scatter(\n",
    "    catalog[\"sky_centroid_win\"].ra.degree,\n",
    "    catalog[\"sky_centroid_win\"].dec.degree,\n",
    "    transform=ax.get_transform('world'),\n",
    "    s=50, # The size of these markers is not related to any measurement apertures!\n",
    "    edgecolor='red', facecolor='none'\n",
    "    )\n",
    "for line in catalog:\n",
    "    ax.text(\n",
    "        x=line[\"sky_centroid_win\"].ra.degree,\n",
    "        y=line[\"sky_centroid_win\"].dec.degree,\n",
    "        s=str(line.index),\n",
    "        transform=ax.get_transform('world'),\n",
    "        color=\"cyan\"\n",
    "        )\n",
    "ax.grid(color='white', ls='solid')\n",
    "ax.coords[0].set_axislabel('RA')\n",
    "ax.coords[1].set_axislabel('Dec')\n",
    "#ax.coords[0].set_ticks(spacing=5.*u.arcmin)\n",
    "#ax.coords[1].set_ticks(spacing=5.*u.arcmin)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The raw light curves\n",
    "\n",
    "Let's now visualize the raw light curves, to get a first impression.\n",
    "The following code is there as an example on how to use and manipulate the `catalog` and how to plot basic light curves, to get you started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We compute an \"instrumental magnitude\" from all our flux measurements.\n",
    "# \"Instrumental\" means that this is not yet calibrated to correspond to an apparent magnitude.\n",
    "# Note that this is the line where we select which aperture to consider.\n",
    "# Of course, you can try to figure out which apertures works best for your data.\n",
    "catalog[\"instr_mag\"] = -2.5 * np.log10(catalog[\"sum_8\"].value) # this is a \"2D\" column: (source index, date)\n",
    "\n",
    "# We can also compute summary statistics for each source. Useful ones could be for example the median instrumental magnitude,\n",
    "# obtained by taking the median along the \"date\"-axis of the instrumental magnitudes:\n",
    "catalog[\"median_instr_mag\"] = np.nanmedian(catalog[\"instr_mag\"].value, axis=1) # this is just a 1D column: (source index)\n",
    "# Using \"nanmedian\" instead of \"median\" has the advantage that NaNs get ignored.\n",
    "# And similarly the standard deviation of each light curve:\n",
    "catalog[\"std_instr_mag\"] = np.nanstd(catalog[\"instr_mag\"].value, axis=1) # this is just a 1D column: (source index)\n",
    "# Note that these are 1D columns: they just have one index, namely the source index.\n",
    "\n",
    "# Another interesting quantity might be the angular separation (in degrees) between each star and your target:\n",
    "target_index = 34 # Adapt this!\n",
    "target_center_pos = catalog[\"sky_centroid_win\"][target_index]\n",
    "catalog[\"separation\"] = astropy.coordinates.SkyCoord.separation(catalog[\"sky_centroid_win\"], target_center_pos)\n",
    "# Depending on your data, it could be important to avoid stars with large angular separation to calibrate your target.\n",
    "\n",
    "# When making a plot (and also to select reference stars), you could hand-pick indices, or build such a list algorithmically:\n",
    "indices_show = [0, 5, 8, 17, 38]\n",
    "\n",
    "plt.figure()\n",
    "ax = plt.subplot()\n",
    "\n",
    "# We simply loop over the indices to show:\n",
    "for index in indices_show:\n",
    "    ax.plot(dates, catalog[\"instr_mag\"][index], lw=1, label=index)\n",
    "\n",
    "ax.invert_yaxis() # Needed, as we show a magnitude on y.\n",
    "\n",
    "# Some advanced settings to help getting a nice format of the date axis labels:\n",
    "ax.xaxis.set_major_formatter(matplotlib.dates.ConciseDateFormatter(ax.xaxis.get_major_locator()))\n",
    "ax.set_xlabel(\"UTC\")\n",
    "ax.set_ylabel(\"Instrumental magnitude\")\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's good to play around with these raw light curves a bit.\n",
    "\n",
    "```{admonition} Question\n",
    "Do you observe any trends or features in these light curves that are common to all (or at least several) stars? Comment on what could cause these flux variations.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration\n",
    "\n",
    "To reveal the transit itself, some empirical calibration of the flux (or magnitude) measured in each exposure is needed. Assuming you work in magnitudes, the basic idea is to subtract some reference light curve from the target light curve. Recall that this corresponds to a division when working with fluxes.\n",
    "\n",
    "To get you started, a somewhat minimal strategy for the calibration could be the following:\n",
    "\n",
    "1. Select a reference star.\n",
    "2. Compute the difference between its light curve (in instrumental magnitudes) and the median of that light curve. This isolates the observed \"variability\" of the reference star.\n",
    "3. Subtract the above difference from the target light curve (in instrumental magnitudes). The resulting light curve is still in instrumental magnitudes, but is now empirically corrected for the variability of the reference star. \n",
    "4. Somewhat optionally, apply a zero point to transform the instrumental magnitudes to (approximate) apparent magnitudes, using for example a literature value of the apparent magnitude of the host star, and the baseline of the transit light curve.\n",
    "\n",
    "This simple approach is probably sufficient to see the transit (depending on its depth and the reference star you selected), but it leaves plenty of room for improvement, especially given our large field of view.\n",
    "\n",
    "```{admonition} Question\n",
    "Try to further develop the above strategy, in particular towards better robustness and precision, write it down in a clear formalism (i.e., equations) with well-chosen symbols, and of course apply it to the data, to discuss any improvements.\n",
    "```\n",
    "\n",
    "```{admonition} Question\n",
    "In theory, what would be the properties of good reference stars to use for the calibration, and why?\n",
    "```\n",
    "\n",
    "It's good to keep in mind that some technical or instrumental effects might possibly have strong effects on the calibration, and influence the optimal selection of reference stars. So in practice, it's mandatory to actually test the calibration, instead of relying on theoretical considerations about what \"should work best\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Discussion of the light curves\n",
    "\n",
    "With a good calibration strategy at hand, you might now be able to clearly reveal the transit.\n",
    "\n",
    "\n",
    "```{admonition} Question\n",
    "In addition to the light curve of the target star, also plot the light curves of some reference stars (applying the same calibration). Use this to verify that your corrections are successful, and that you did not pick up variable stars or stars that have observational issues as reference sources.\n",
    "Do you see residual correlations between some stars?\n",
    "```\n",
    "\n",
    "```{note}\n",
    "It could be interesting to make a scatter plot of the median apparent magnitude versus the standard deviation of each calibrated light curve (i.e., star). This could help you identify which stars might not be that useful for the calibration (e.g., pay attention to potential saturation issues for bright stars), and also it nicely summarizes the achieved photometric precision as function of magnitude. \n",
    "```\n",
    "\n",
    "```{admonition} Question\n",
    "If you see stars with strong residuals after the calibration, what could be the reasons?\n",
    "You can try to optimize the calibration by adapting the code to ignore particular stars.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Discussion of the exoplanet transit\n",
    "\n",
    "```{admonition} Question\n",
    "Can you identify the start and/or end of the transit?\n",
    "Give an estimate for the uncertainty of your estimate.\n",
    "```\n",
    "\n",
    "```{admonition} Question\n",
    "Can you measure a depth (in magnitude) of the transit? If yes, convert your value to the commonly used \"parts per thousand\" (ppt).\n",
    "```\n",
    "\n",
    "```{admonition} Question\n",
    "Are your findings regarding the transit consistent with measurements from the literature (depth, duration, start/end time of the transit)?\n",
    "```\n",
    "\n",
    "```{admonition} Question\n",
    "Calculate the radius of the exoplanet relative to its host star from your measurement of the light curve. Think as well of how you could estimate the uncertainties.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datared2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
